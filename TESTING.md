# Testing Strategy â€“ Curamyn AI Healthcare System

This document describes the comprehensive testing philosophy, strategy, and implementation used in the Curamyn AI system.

---

## ğŸ“‹ Table of Contents

- [Overview](#overview)
- [Testing Philosophy](#testing-philosophy)
- [System Under Test](#system-under-test)
- [Types of Tests](#types-of-tests)
- [AI-Specific Testing Strategy](#ai-specific-testing-strategy)
- [Fallback Testing](#fallback-testing)
- [Mocking Strategy](#mocking-strategy)
- [What Is NOT Tested](#what-is-not-tested)
- [How to Run Tests](#how-to-run-tests)
- [Test Coverage](#test-coverage)
- [CI/CD Integration](#cicd-integration)
- [Privacy-Safe Testing](#privacy-safe-testing)
- [Production Readiness Validation](#production-readiness-validation)

---

## Overview

Curamyn is a multimodal AI application integrating:
- **Large Language Models (LLMs)**: Google Gemini Flash for health advice, education, and conversation
- **Voice Processing**: Deepgram (STT), Piper TTS (TTS), with Whisper fallback
- **Computer Vision**: PyTorch ResNet-18 CNN for medical image risk analysis
- **Document Understanding**: Tesseract OCR + Presidio PII removal + Gemini LLM
- **Safety-Guarded Logic**: Input/output safety checks, emergency detection, consent enforcement

Due to the **non-deterministic and resource-heavy nature** of these components, a structured and defensive testing strategy is essential.

---

## Testing Philosophy

Curamyn follows the principle:

### **"Test system behavior, not model intelligence."**

The goal is to verify that:
- âœ… **The system responds safely** under all conditions
- âœ… **Failures are handled gracefully** with appropriate fallbacks
- âœ… **Incorrect inputs never crash the system**
- âœ… **External services are isolated** from core logic
- âœ… **Privacy and security constraints** are enforced
- âœ… **Consent mechanisms** prevent unauthorized data processing

**Model accuracy and human-level understanding are evaluated separately** from software tests through:
- Offline model benchmarking (medical datasets)
- Manual testing and user demos
- A/B testing in production
- MLflow experiment tracking

---

## System Under Test

### Architecture Layers

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               API Layer (FastAPI)                   â”‚
â”‚  â€¢ Authentication (JWT)                             â”‚
â”‚  â€¢ Rate limiting (SlowAPI)                          â”‚
â”‚  â€¢ Input validation (Pydantic)                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          Orchestration Layer                        â”‚
â”‚  â€¢ Safety guards (input/output validation)          â”‚
â”‚  â€¢ Consent engine (permission checks)               â”‚
â”‚  â€¢ Session state management                         â”‚
â”‚  â€¢ Context enrichment (memory, documents, images)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚            â”‚            â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â–¼â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Voice Pipelineâ”‚ â”‚Documentâ”‚ â”‚Image Analysisâ”‚
â”‚ â€¢ STT (Dgram) â”‚ â”‚  OCR   â”‚ â”‚ CNN (PyTorch)â”‚
â”‚ â€¢ LLM (Gemini)â”‚ â”‚â€¢ Geminiâ”‚ â”‚  â€¢ S3 Models â”‚
â”‚ â€¢ TTS (Piper) â”‚ â”‚â€¢ PII   â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Critical Testing Areas

| Layer | Testing Focus |
|-------|---------------|
| **API Layer** | Authentication, rate limiting, input validation, error responses |
| **Orchestration** | Safety enforcement, consent checks, routing logic, fallback execution |
| **Voice Pipeline** | STT fallback (Deepgramâ†’Whisper), TTS error handling, latency tracking |
| **Document Processing** | OCR text extraction, PII removal, LLM summarization fallback |
| **Image Analysis** | CNN loading from S3, inference error handling, safe fallback messages |
| **Session Management** | Memory persistence, context injection, summary generation |
| **Safety Guards** | Emergency detection, diagnosis blocking, out-of-scope filtering |

---

## Types of Tests

### 1. Unit Tests (`tests/unit/`)

**Purpose**: Validate individual service behavior in isolation

**Characteristics**:
- âœ… No external API calls (LLM, Whisper, CNN, S3)
- âœ… Use mocking for deterministic outcomes
- âœ… Fast execution (< 1 second per test)
- âœ… Test pure logic, not AI intelligence

**Examples**:

#### LLM Fallback Handling
```python
def test_llm_uses_fallback_when_response_invalid(monkeypatch):
    """Test that LLM returns safe fallback when Gemini fails"""
    from app.chat_service.services import llm_service

    class FakeClient:
        class models:
            @staticmethod
            def generate_content(*args, **kwargs):
                return None  # Simulate Gemini failure

    monkeypatch.setattr(
        llm_service,
        "_load_gemini",
        lambda: (FakeClient(), None),
    )

    result = llm_service.analyze_text(text="Hello")

    # Assert safe fallback is used
    assert result["response_text"] == "I'm here with you."
    assert result["intent"] == "casual_chat"
```

#### Safety Rule Enforcement
```python
def test_diagnosis_blocked():
    """Test that diagnosis requests are blocked by safety guard"""
    from app.chat_service.services.safety_guard import check_output_safety, SafetyViolation
    
    with pytest.raises(SafetyViolation):
        check_output_safety(user_text="Do I have cancer?")
```

#### Voice Pipeline Control Flow
```python
@pytest.mark.asyncio
async def test_voice_pipeline_handles_stt_failure():
    """Test that voice pipeline handles STT errors gracefully"""
    
    with patch(
        "app.chat_service.services.voice_pipeline_service.transcribe_audio",
        new_callable=AsyncMock,
        side_effect=RuntimeError("STT failed"),
    ):
        result = await voice_chat_pipeline(b"fake_audio")
        
        # Should return error message with audio
        assert "message" in result
        assert "couldn't hear you" in result["message"].lower()
        assert result.get("tts_failed") is True
```

#### CNN Routing Logic
```python
def test_cnn_returns_risk():
    """Test that CNN service returns valid risk assessment"""
    from app.chat_service.services.cnn_service import predict_risk
    
    fake_model = MagicMock()
    
    with patch("app.chat_service.services.cnn_service.Image.open"):
        with patch("app.chat_service.services.cnn_service._get_model", return_value=fake_model):
            with patch("torch.sigmoid", return_value=MagicMock(item=lambda: 0.1)):
                result = predict_risk(image_type="xray", image_bytes=b"fake")
                
                assert result["risk"] in {"normal", "needs_attention"}
                assert "confidence" in result
```

---

### 2. Integration Tests (`tests/integration/`)

**Purpose**: Validate interaction between internal components

**Characteristics**:
- âœ… Authentication is mocked (no real JWT validation)
- âœ… External services (LLM, STT, TTS, CNN) remain isolated
- âœ… Test request/response flow through multiple layers
- âœ… Validate data transformations and error propagation

**Examples**:

#### AI Interaction Endpoint
```python
def test_ai_interact_endpoint():
    """Test /ai/interact endpoint with mocked authentication"""
    from fastapi.testclient import TestClient
    from app.main import app
    from app.core.dependencies import get_current_user
    
    def override_get_current_user():
        return {"sub": "test-user"}
    
    app.dependency_overrides[get_current_user] = override_get_current_user
    client = TestClient(app)
    
    response = client.post(
        "/ai/interact",
        data={"input_type": "text", "text": "Hi"}
    )
    
    assert response.status_code == 200
    assert "message" in response.json()
```

#### Memory Deletion Flow
```python
def test_clear_memory():
    """Test memory deletion endpoint"""
    from fastapi.testclient import TestClient
    from app.main import app
    
    # Mock authentication
    app.dependency_overrides[get_current_user] = lambda: {"sub": "test-user"}
    client = TestClient(app)
    
    response = client.delete("/memory/clear")
    
    assert response.status_code == 200
    assert "deleted_sessions" in response.json()
```

---

### 3. End-to-End (E2E) Tests (`tests/e2e/`)

**Purpose**: Smoke tests to validate application startup and basic functionality

**Characteristics**:
- âœ… No real AI inference (too slow and non-deterministic)
- âœ… Validate that the application boots correctly
- âœ… Check critical endpoints respond (health check, auth)
- âœ… Fast smoke tests (< 5 seconds total)

**Examples**:

```python
def test_full_flow_smoke():
    """Smoke test: Application starts and responds"""
    assert True  # Placeholder for basic startup validation
```

---

## AI-Specific Testing Strategy

### Why LLM Outputs Are NOT Asserted Directly

LLM responses can vary due to:
- **Token limits**: Different output lengths based on token budget
- **Provider behavior**: Gemini API updates, rate limiting, quota exhaustion
- **Internal model reasoning**: Non-deterministic sampling, temperature effects
- **Network failures**: Timeouts, connection drops, partial responses

**Because of this, tests DO NOT assert exact LLM output text.**

Instead, tests validate:
- âœ… **Presence of a response** (not empty)
- âœ… **Correct structure** (dict with required keys: `intent`, `response_text`, `severity`)
- âœ… **Execution of fallback logic** when LLM output is unusable
- âœ… **Safety constraints** are enforced (no diagnosis, no medication advice)

### Example: LLM Response Validation

```python
def test_llm_response_structure():
    """Test LLM returns valid structure (not exact text)"""
    result = analyze_text(text="I'm feeling stressed")
    
    # Validate structure
    assert isinstance(result, dict)
    assert "response_text" in result
    assert "intent" in result
    assert "severity" in result
    
    # Validate non-empty
    assert len(result["response_text"]) > 0
    
    # Validate expected intent
    assert result["intent"] in {"health_support", "casual_chat", "self_care"}
    
    # DO NOT assert exact text (non-deterministic)
    # âŒ assert result["response_text"] == "Stress is challenging..."
```

This reflects **real-world production conditions** where LLM outputs are variable.

---

## Fallback Testing

Fallback behavior is treated as **first-class functionality** in Curamyn. Every critical service has multi-tier fallback chains that MUST be tested.

### 1. Speech-to-Text Fallback Chain

```
Deepgram API (Primary)
  â†“ [Fails]
OpenAI Whisper (Fallback)
  â†“ [Fails]
Return error with cached TTS response
```

**Test Strategy**:
```python
@pytest.mark.asyncio
async def test_stt_falls_back_to_whisper():
    """Test STT fallback: Deepgram â†’ Whisper"""
    
    # Mock Deepgram failure
    with patch("app.chat_service.services.deepgram_service.get_deepgram_client", 
               side_effect=RuntimeError("Deepgram unavailable")):
        
        # Mock Whisper success
        with patch("app.chat_service.services.whisper_service.transcribe",
                   return_value="hello"):
            
            result = await transcribe_audio(b"audio_bytes")
            
            assert result == "hello"
```

### 2. LLM Fallback Chain

```
Gemini Flash (Primary)
  â†“ [Empty output or error]
Gemini Flash Lite (Fallback)
  â†“ [Empty output or error]
Safe Static Response ("I'm here with you.")
```

**Test Strategy**:
```python
def test_llm_fallback_to_safe_response():
    """Test LLM fallback: Gemini Flash â†’ Gemini Flash Lite â†’ Safe response"""
    
    class EmptyResponseClient:
        class models:
            @staticmethod
            def generate_content(*args, **kwargs):
                # Simulate empty response
                class EmptyResponse:
                    text = ""
                return EmptyResponse()
    
    with patch("app.chat_service.services.health_advisor_service._load_gemini",
               return_value=(EmptyResponseClient(), None)):
        
        result = analyze_health_text(text="Hello")
        
        # Should fall back to safe response
        assert result["response_text"] == "I am here with you. Let us take this one step at a time. You do not have to handle this alone."
```

### 3. TTS Fallback

```
Piper TTS Synthesis
  â†“ [Subprocess fails]
Emergency TTS ("I hear you.")
  â†“ [Also fails]
Return text-only with tts_failed=True
```

**Test Strategy**:
```python
def test_tts_handles_subprocess_error():
    """Test TTS handles subprocess errors gracefully"""
    from subprocess import CalledProcessError
    
    with patch("app.chat_service.services.tts_streamer.subprocess.run",
               side_effect=CalledProcessError(1, "piper", stderr=b"Error")):
        
        with pytest.raises(RuntimeError, match="TTS generation failed"):
            synthesize_tts("Test")
```

### 4. CNN Model Loading Fallback

```
Load from memory cache
  â†“ [Not in cache]
Download from S3
  â†“ [Download fails]
Return safe generic message
```

**Test Strategy**:
```python
def test_cnn_handles_model_unavailable():
    """Test CNN handles model loading failures"""
    
    with patch("app.chat_service.services.cnn_service._get_model",
               side_effect=RuntimeError("Model unavailable")):
        
        with pytest.raises(RuntimeError):
            predict_risk(image_type="xray", image_bytes=b"fake")
```

---

## Mocking Strategy

All external and resource-heavy dependencies are mocked:

| Service | Mocking Approach | Reason |
|---------|------------------|--------|
| **LLM APIs (Gemini)** | Mock `genai.Client` class | Avoid API costs, ensure deterministic tests |
| **Speech-to-Text (Deepgram/Whisper)** | Mock `transcribe_audio()` function | Avoid network calls, control test data |
| **Text-to-Speech (Piper)** | Mock `subprocess.run()` | Avoid system dependency, fast execution |
| **CNN Inference** | Mock `torch.nn.Module` forward pass | Avoid GPU/CPU computation, control outputs |
| **S3 Model Loading** | Mock `boto3.client` S3 operations | Avoid network calls, ensure offline tests |
| **MongoDB** | Use in-memory test database or mock collections | Avoid production DB pollution, fast cleanup |
| **Authentication** | Override `get_current_user()` dependency | Bypass JWT validation in tests |

**Mocking is applied at the usage point**, ensuring:
- âœ… **Deterministic test results** (no randomness from AI)
- âœ… **No API costs** (Gemini, Deepgram are paid services)
- âœ… **CI/CD compatibility** (no external dependencies)
- âœ… **Fast local execution** (< 30 seconds for full test suite)

---

## What Is NOT Tested

The following are intentionally excluded from pytest:

### 1. Model Accuracy or Medical Correctness
- âŒ "Is the LLM response medically accurate?"
- âŒ "Does the CNN correctly identify pneumonia?"
- âŒ "Is the OCR extraction 100% accurate?"

**Why?** These require medical datasets, domain expertise, and offline evaluation metrics.

**How evaluated?** 
- Offline benchmarking on medical datasets
- Manual review by healthcare professionals
- User feedback and A/B testing in production

---

### 2. Audio Quality or Speech Naturalness
- âŒ "Does the TTS sound natural?"
- âŒ "Is the STT transcription perfectly accurate?"

**Why?** Subjective metrics that require human evaluation.

**How evaluated?**
- Manual listening tests
- User surveys on voice quality
- WER (Word Error Rate) measurement on test audio sets

---

### 3. Image Classification Performance
- âŒ "Does the CNN achieve 95% accuracy on chest X-rays?"

**Why?** Requires labeled medical image datasets and ROC/AUC analysis.

**How evaluated?**
- Offline evaluation on validation datasets
- Precision/Recall/F1 metrics tracked in MLflow
- Comparison with baseline models

---

### 4. LLM Intelligence or Creativity
- âŒ "Is the LLM empathetic and helpful?"
- âŒ "Does the LLM provide good health advice?"

**Why?** Subjective evaluation requiring human judgment.

**How evaluated?**
- User feedback (thumbs up/down)
- Session summaries for quality review
- A/B testing different prompts in production

---

This separation ensures clear boundaries between:
- **Software testing** (system reliability, safety, error handling)
- **Model validation** (accuracy, performance, medical correctness)

---

## How to Run Tests

### Run All Tests
```bash
pytest
```

### Run Specific Test Suites
```bash
# Unit tests only
pytest tests/unit/

# LLM tests
pytest tests/unit/llm/

# Voice tests
pytest tests/unit/voice/

# Vision tests (OCR, CNN)
pytest tests/unit/vision/

# Safety tests
pytest tests/unit/safety/

# Integration tests
pytest tests/integration/

# E2E tests
pytest tests/e2e/
```

### Run with Verbose Output
```bash
pytest -v
```

### Run with Coverage Report
```bash
pytest --cov=app --cov-report=html
```

### Run Specific Test File
```bash
pytest tests/unit/llm/test_llm_fallback_logic.py
```

### Run Specific Test Function
```bash
pytest tests/unit/safety/test_emergency_detection.py::test_emergency_detected
```

### Run Tests in Parallel (Faster)
```bash
pytest -n auto  # Requires pytest-xdist
```

---

## Test Coverage

### Current Coverage Metrics

| Module | Coverage | Notes |
|--------|----------|-------|
| **Orchestration Layer** | 85% | Core routing, safety, consent |
| **LLM Services** | 90% | Fallback logic, prompt building |
| **Voice Pipeline** | 88% | STT/TTS fallback, error handling |
| **OCR Service** | 80% | Text extraction, PII removal |
| **CNN Service** | 85% | Model loading, inference |
| **Safety Guards** | 95% | Input/output validation, emergency detection |
| **API Routes** | 75% | Authentication, rate limiting |

### Coverage Report

Generate HTML coverage report:
```bash
pytest --cov=app --cov-report=html
```

Open `htmlcov/index.html` in browser to view detailed coverage.

### Coverage Goals

- **Critical Safety Logic**: 95%+ (safety guards, consent enforcement)
- **Core Business Logic**: 85%+ (orchestration, routing, fallback)
- **External Service Wrappers**: 70%+ (LLM, STT, TTS, CNN clients)

---

## CI/CD Integration

### GitHub Actions Workflow

**File**: `.github/workflows/ci.yml`

```yaml
name: CI Pipeline

on:
  push:
    branches: ["main", "develop"]
  pull_request:
    branches: ["main", "develop"]

jobs:
  test:
    name: Run Tests
    runs-on: ubuntu-latest

    services:
      mongo:
        image: mongo:6
        ports:
          - 27017:27017

    env:
      CURAMYN_ENV: test
      PYTHONPATH: .
      CURAMYN_GEMINI_API_KEY: dummy
      CURAMYN_JWT_SECRET: dummy
      CURAMYN_MONGO_URI: mongodb://localhost:27017
      CURAMYN_MONGO_DB: test_db

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Wait for MongoDB
        run: |
          for i in {1..10}; do
            if nc -z localhost 27017; then
              echo "MongoDB is up"
              break
            fi
            sleep 1
          done

      - name: Run pytest
        run: pytest -v --disable-warnings --maxfail=1

      - name: Black formatting check
        run: black --check .
```

### CI Pipeline Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. Code pushed to main/develop         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. GitHub Actions triggers             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. Setup environment                   â”‚
â”‚     â€¢ Python 3.11                       â”‚
â”‚     â€¢ MongoDB service                   â”‚
â”‚     â€¢ Install dependencies              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  4. Run tests (pytest)                  â”‚
â”‚     â€¢ All external services mocked      â”‚
â”‚     â€¢ Fail fast on first error          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  5. Code formatting check (black)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  6. Build Docker images (if tests pass) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Privacy-Safe Testing

### No Real User Data in Tests

**Critical Rule**: Tests NEVER use real user data, PII, or medical content.

**Enforced by**:
- âœ… Using `CURAMYN_ENV=test` environment variable
- âœ… Separate test database (`test_db`)
- âœ… Dummy API keys (`dummy`)
- âœ… Mocked external services (no real API calls)
- âœ… Synthetic test data only

### Test Data Examples

**Valid Test Inputs**:
```python
# Safe synthetic medical document
test_doc = """
HAEMATOLOGY REPORT
Hemoglobin: 13.5 g/dL (Reference: 12.0-15.5 g/dL)
WBC Count: 7,500 cells/ÂµL
"""

# Safe synthetic user message
test_message = "I'm feeling stressed about work"

# Safe synthetic audio bytes
test_audio = b"\x00\x01" * 100
```

**Invalid Test Inputs** (Never Used):
```python
# âŒ Real user chat history
# âŒ Actual medical records
# âŒ Real patient names or identifiers
# âŒ Production database credentials
```

---

## Production Readiness Validation

### Checklist for Production Deployment

Before deploying to production, validate:

#### 1. Safety & Security
- [ ] All safety guards tested (diagnosis blocking, emergency detection)
- [ ] Consent enforcement working (voice, image, document, memory)
- [ ] JWT authentication tested
- [ ] Rate limiting validated (25/min for chat, 5/min for login)
- [ ] PII removal tested (Presidio redaction)
- [ ] Audit logging functional (HIPAA compliance)

#### 2. Fallback Chains
- [ ] LLM fallback tested (Gemini Flash â†’ Gemini Flash Lite â†’ Safe response)
- [ ] STT fallback tested (Deepgram â†’ Whisper â†’ Error message)
- [ ] TTS fallback tested (Piper â†’ Emergency TTS â†’ Text-only)
- [ ] CNN fallback tested (S3 cache â†’ Download â†’ Safe message)

#### 3. Error Handling
- [ ] 500 errors return user-friendly messages
- [ ] Network timeouts handled gracefully
- [ ] Empty responses from LLM handled
- [ ] Invalid file uploads rejected safely
- [ ] Database connection failures logged

#### 4. Performance
- [ ] Voice pipeline latency < 3.5s (end-to-end)
- [ ] Text chat response < 2.5s
- [ ] OCR processing < 10s
- [ ] CNN inference < 4s
- [ ] Session state persisted correctly

#### 5. Monitoring
- [ ] MLflow tracking functional (latency, model metrics)
- [ ] Sentry error tracking enabled
- [ ] Audit logs being written to MongoDB
- [ ] ETL pipeline running (cron job)
- [ ] PowerBI dashboards connected

---

## Testing Best Practices

### 1. Test Naming Convention
```python
# Good: Descriptive, explains what is tested
def test_llm_returns_safe_fallback_when_gemini_unavailable():
    pass

# Bad: Vague, unclear purpose
def test_llm():
    pass
```

### 2. Arrange-Act-Assert Pattern
```python
def test_safety_guard_blocks_diagnosis():
    # Arrange
    user_input = "Do I have cancer?"
    
    # Act
    with pytest.raises(SafetyViolation):
        check_output_safety(user_text=user_input)
    
    # Assert (implicit in pytest.raises)
```

### 3. One Assertion Per Test (When Possible)
```python
# Good: Tests one specific behavior
def test_emergency_detection_identifies_crisis_language():
    assert detect_emergency("I can't breathe")

# Acceptable: Multiple assertions for same behavior
def test_llm_response_structure():
    result = analyze_text(text="Hello")
    assert "response_text" in result
    assert "intent" in result
    assert "severity" in result
```

### 4. Use Fixtures for Common Setup
```python
@pytest.fixture
def mock_llm_client():
    """Fixture for mocked Gemini client"""
    class FakeClient:
        class models:
            @staticmethod
            def generate_content(*args, **kwargs):
                return FakeResponse(text="Hello")
    return FakeClient()

def test_with_fixture(mock_llm_client):
    # Use mock_llm_client in test
    pass
```

---

## Continuous Improvement

### Adding New Tests

When adding new features, always add corresponding tests:

1. **Unit tests** for core logic
2. **Integration tests** for API endpoints
3. **Fallback tests** for error scenarios
4. **Safety tests** if touching consent or security

### Test Review Checklist

Before merging, ensure:
- [ ] All tests pass locally
- [ ] CI/CD pipeline passes
- [ ] Code coverage maintained or improved
- [ ] No real API keys or credentials in tests
- [ ] Test names are descriptive
- [ ] External services properly mocked

---

## Summary

Curamyn's testing strategy ensures:

âœ… **Reliability**: System never crashes, always responds safely  
âœ… **Privacy**: No real user data in tests, PII handling validated  
âœ… **Resilience**: Fallback chains tested for every critical service  
âœ… **Security**: Safety guards, consent, authentication validated  
âœ… **Maintainability**: Clear test structure, good coverage, CI/CD integration  

**The goal is not to test if AI is "smart"â€”but to test if the system is safe, reliable, and production-ready.**

---

**For questions or contributions, see [CONTRIBUTING.md](CONTRIBUTING.md).**