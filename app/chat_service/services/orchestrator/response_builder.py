"""
Response builder.

Constructs final user-facing responses based on
LLM output, context, consent, and response mode.
"""

import base64
from typing import Dict, Any

from app.chat_service.services.tts_service import text_to_speech
from app.chat_service.services.voice_pipeline_service import (
    finalize_spoken_text,
)
from app.chat_service.utils.logger import get_logger

logger = get_logger(__name__)


def build_response(
    *,
    llm_result: Dict[str, Any],
    context: Dict[str, Any],
    response_mode: str,
    consent: Dict[str, bool],
) -> Dict[str, Any]:
    """
    Build the final API response payload.

    Args:
        llm_result (dict): Output from LLM or analysis service.
        context (dict): Contextual data (e.g. image analysis).
        response_mode (str): 'text' or 'voice'.
        consent (dict): User consent flags.

    Returns:
        dict: Final response payload.
    """
    intent = llm_result.get("intent")
    image_analysis = context.get("image_analysis")

    logger.debug(
        "Building response",
        extra={"intent": intent, "response_mode": response_mode},
    )

    # ---------- IMAGE (CNN) RESPONSE ---------- #
    if image_analysis:
        risk = image_analysis.get("risk", "unknown")
        message = _risk_message(risk)

        response = {
            "message": message,
            "disclaimer": (
                "This information is generated by an AI system and is for informational "
                "purposes only. It is not a medical diagnosis. "
                "Please consult a healthcare professional."
            ),
        }

        if response_mode == "voice" and consent.get("voice"):
            logger.debug("Adding voice output to image response")
            audio_bytes = text_to_speech(message)
            response["audio"] = base64.b64encode(audio_bytes).decode("utf-8")

        return response

    # ---------- OCR DOCUMENT RESPONSE ---------- #
    if intent == "document_understanding":
        return {
            "message": llm_result.get(
                "response_text",
                "I was able to read the document, but could not summarize it clearly.",
            )
        }

    # ---------- HEALTH ADVISOR ---------- #
    if intent == "health_advice":
        return {
            "message": llm_result.get("response_text", "I'm here to help."),
        }

    # ---------- GENERAL / VOICE CHAT ---------- #
    raw_text = llm_result.get("response_text", "I'm here with you.")

    message = finalize_spoken_text(
        raw_text,
        llm_result.get("severity", "low"),
    )

    response: Dict[str, Any] = {"message": message}

    if response_mode == "voice" and consent.get("voice"):
        logger.debug("Adding voice output to chat response")
        audio_bytes = text_to_speech(message)
        response["audio"] = base64.b64encode(audio_bytes).decode("utf-8")

    return response


def _risk_message(risk: str) -> str:
    """
    Translate CNN risk classification into a user-safe message.

    Args:
        risk (str): Risk label from image model.

    Returns:
        str: Human-readable explanation.
    """
    if risk == "needs_attention":
        return (
            "This scan shows patterns that may require medical attention. "
            "Please consult a healthcare professional."
        )

    return "No critical abnormalities were detected."

